{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4dNRn3OgQNhc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pooja Sharma\\AppData\\Local\\Temp\\ipykernel_8052\\2060519318.py:13: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  get_ipython().magic('matplotlib inline')\n"
     ]
    }
   ],
   "source": [
    "#Install all the Reqiuired Libraries and Packages \n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "from python_speech_features import mfcc , logfbank\n",
    "import librosa\n",
    "import pickle\n",
    "from scipy import signal\n",
    "get_ipython().magic('matplotlib inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "smWEZT7EXSoT"
   },
   "outputs": [],
   "source": [
    "#Import all the Required Packages and Libraies.\n",
    "import soundfile\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0PkasViUY6ZJ",
    "outputId": "929414a6-7020-45cc-bb22-f2519c5b8638"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path=r'Dataset')\n",
    "def getListOfFiles(dirName):\n",
    "    listOfFile=os.listdir(dirName)\n",
    "    allFiles=list()\n",
    "    for entry in listOfFile:\n",
    "        fullPath=os.path.join(dirName, entry)\n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles=allFiles + getListOfFiles(fullPath)\n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "    return allFiles\n",
    "\n",
    "dirName = r'Dataset'\n",
    "listOfFiles = getListOfFiles(dirName)\n",
    "len(listOfFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uYFLdqvzY_1t"
   },
   "outputs": [],
   "source": [
    "def envelope(y , rate, threshold):\n",
    "    mask=[]\n",
    "    y=pd.Series(y).apply(np.abs)\n",
    "    y_mean = y.rolling(window=int(rate/10) ,  min_periods=1 , center = True).mean()\n",
    "    for mean in y_mean:\n",
    "        if mean>threshold:\n",
    "            mask.append(True)\n",
    "        else:\n",
    "            mask.append(False)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5ae56fce"
   },
   "outputs": [],
   "source": [
    "#Next Step is In-Depth Visualisation of Audio Fiels and its certain features to plot for.\n",
    "#They are the Plotting Functions to be called later. \n",
    "def plot_signals(signals):\n",
    "    fig , axes = plt.subplots(nrows=2, ncols=5,sharex =False , sharey=True, figsize=(20,5))\n",
    "    fig.suptitle('Time Series' , size=16)\n",
    "    i=0\n",
    "    for x in range(2):\n",
    "        for y in range(5):\n",
    "            axes[x,y].set_title(list(signals.keys())[i])\n",
    "            axes[x,y].plot(list(signals.values())[i])\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "            i +=1\n",
    "\n",
    "def plot_fft(fft):\n",
    "    fig , axes = plt.subplots(nrows=2, ncols=5,sharex =False , sharey=True, figsize=(20,5))\n",
    "    fig.suptitle('Fourier Transform' , size=16)\n",
    "    i=0\n",
    "    for x in range(2):\n",
    "        for y in range(5):\n",
    "            data = list(fft.values())[i]\n",
    "            Y,freq = data[0] , data[1]\n",
    "            axes[x,y].set_title(list(fft.keys())[i])\n",
    "            axes[x,y].plot(freq , Y)\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "            i +=1\n",
    "    \n",
    "def plot_fbank(fbank):\n",
    "    fig , axes = plt.subplots(nrows=2, ncols=5,sharex =False , sharey=True, figsize=(20,5))\n",
    "    fig.suptitle('Filter Bank Coefficients' , size=16)\n",
    "    i=0\n",
    "    for x in range(2):\n",
    "        for y in range(5):\n",
    "            axes[x,y].set_title(list(fbank.keys())[i])\n",
    "            axes[x,y].imshow(list(fbank.values())[i],cmap='hot', interpolation = 'nearest')\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "            i +=1\n",
    "            \n",
    "def plot_mfccs(mfccs):\n",
    "    fig , axes = plt.subplots(nrows=2, ncols=5,sharex =False , sharey=True, figsize=(20,5))\n",
    "    fig.suptitle('Mel Frequency Capstrum  Coefficients' , size=16)\n",
    "    i=0\n",
    "    for x in range(2):\n",
    "        for y in range(5):\n",
    "            axes[x,y].set_title(list(mfccs.keys())[i])\n",
    "            axes[x,y].imshow(list(mfccs.values())[i],\n",
    "                             cmap='hot', interpolation = 'nearest')\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "            i +=1\n",
    "\n",
    "def calc_fft(y,rate):\n",
    "    n = len(y)\n",
    "    freq = np.fft.rfftfreq(n , d= 1/rate)\n",
    "    Y= abs(np.fft.rfft(y)/n)\n",
    "    return(Y,freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lHoohhHPcDKj"
   },
   "outputs": [],
   "source": [
    "#Now Cleaning Step is Performed where:\n",
    "#DOWN SAMPLING OF AUDIO FILES IS DONE  AND PUT MASK OVER IT AND DIRECT INTO CLEAN FOLDER\n",
    "#MASK IS TO REMOVE UNNECESSARY EMPTY VOIVES AROUND THE MAIN AUDIO VOICE \n",
    "def envelope(y , rate, threshold):\n",
    "    mask=[]\n",
    "    y=pd.Series(y).apply(np.abs)\n",
    "    y_mean = y.rolling(window=int(rate/10) ,  min_periods=1 , center = True).mean()\n",
    "    for mean in y_mean:\n",
    "        if mean>threshold:\n",
    "            mask.append(True)\n",
    "        else:\n",
    "            mask.append(False)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UNlqWR_WcKRi",
    "outputId": "e65e3fb0-ac7e-4dba-dd1a-35cf719bcf5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [02:17<00:00,  4.38it/s]\n"
     ]
    }
   ],
   "source": [
    "#The clean Audio Files are redirected to Clean Audio Folder Directory \n",
    "import glob,pickle\n",
    "for file in tqdm(glob.glob(r'Dataset\\\\**\\\\*.wav')):\n",
    "    file_name = os.path.basename(file)\n",
    "    signal , rate = librosa.load(file, sr=16000)\n",
    "    mask = envelope(signal,rate, 0.0005)\n",
    "    wavfile.write(filename= 'clean_speech\\\\'+str(file_name), rate=rate,data=signal[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Lf8vja7TcPBI"
   },
   "outputs": [],
   "source": [
    "#Feature Extraction of Audio Files Function \n",
    "#Extract features (mfcc, chroma, mel) from a sound file\n",
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate=sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft=np.abs(librosa.stft(X))\n",
    "        result=np.array([])\n",
    "        if mfcc:\n",
    "            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result=np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result=np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "        result=np.hstack((result, mel))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "a0Rbqh6yQI8l"
   },
   "outputs": [],
   "source": [
    "animals = {\n",
    "    'cat' : 'Cat',\n",
    "    'dog' : 'Dog',\n",
    "    'cow' : 'Cow',\n",
    "    'donkey' : 'Donkey',\n",
    "    'monkey' : 'Monkey',\n",
    "    'sheep' : 'Sheep'}\n",
    "\n",
    "observed_animals = ['Cat','Dog','Cow','Donkey','Monkey','Sheep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FoteAcWjoy09"
   },
   "outputs": [],
   "source": [
    "#Load the data and extract features for each sound file\n",
    "from glob import glob\n",
    "import os\n",
    "import glob\n",
    "def load_data(test_size=0.33):\n",
    "    x,y=[],[]\n",
    "    answer = 0\n",
    "    for file in glob.glob(r'C:\\Users\\Pooja Sharma\\Documents\\Animal Voice Classification\\clean_speech/*.wav'):\n",
    "        file_name=os.path.basename(file)\n",
    "        animal=animals[file_name.split(\"-\")[0]]\n",
    "        feature=extract_feature(file, mfcc=True, chroma=True, mel=False)\n",
    "        x.append(feature)\n",
    "        y.append([animal,file_name])\n",
    "          \n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hSVGiP4LrP1V",
    "outputId": "d70673fd-4d70-4a1c-8fce-5ff66ceadb13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 53) (150, 53) (450, 2) (150, 2)\n",
      "(450,) (150,)\n",
      "sheep-38.wav\n",
      "cow-86.wav\n",
      "cat-63.wav\n",
      "cow-46.wav\n",
      "sheep-55.wav\n",
      "cat-59.wav\n",
      "monkey-76.wav\n",
      "cat-54.wav\n",
      "cat-65.wav\n",
      "monkey-51.wav\n",
      "dog-7.wav\n",
      "monkey-28.wav\n",
      "cow-3.wav\n",
      "donkey-28.wav\n",
      "sheep-86.wav\n",
      "donkey-29.wav\n",
      "dog-73.wav\n",
      "cat-38.wav\n",
      "sheep-91.wav\n",
      "cow-9.wav\n",
      "cat-45.wav\n",
      "monkey-31.wav\n",
      "sheep-93.wav\n",
      "cat-44.wav\n",
      "dog-61.wav\n",
      "monkey-43.wav\n",
      "sheep-99.wav\n",
      "cat-79.wav\n",
      "sheep-49.wav\n",
      "sheep-46.wav\n",
      "cow-77.wav\n",
      "cat-68.wav\n",
      "cat-42.wav\n",
      "dog-19.wav\n",
      "dog-9.wav\n",
      "dog-18.wav\n",
      "dog-89.wav\n",
      "donkey-53.wav\n",
      "cow-52.wav\n",
      "cow-72.wav\n",
      "monkey-30.wav\n",
      "sheep-76.wav\n",
      "cat-24.wav\n",
      "cow-14.wav\n",
      "monkey-94.wav\n",
      "cow-5.wav\n",
      "donkey-7.wav\n",
      "cat-94.wav\n",
      "donkey-1.wav\n",
      "donkey-92.wav\n",
      "cat-36.wav\n",
      "cat-58.wav\n",
      "donkey-84.wav\n",
      "cow-92.wav\n",
      "sheep-51.wav\n",
      "sheep-21.wav\n",
      "sheep-27.wav\n",
      "dog-59.wav\n",
      "sheep-16.wav\n",
      "sheep-71.wav\n",
      "dog-3.wav\n",
      "dog-56.wav\n",
      "cat-98.wav\n",
      "cat-100.wav\n",
      "cat-97.wav\n",
      "monkey-84.wav\n",
      "cat-74.wav\n",
      "sheep-13.wav\n",
      "cat-91.wav\n",
      "cow-23.wav\n",
      "donkey-36.wav\n",
      "dog-75.wav\n",
      "dog-99.wav\n",
      "donkey-91.wav\n",
      "donkey-62.wav\n",
      "donkey-88.wav\n",
      "cow-44.wav\n",
      "dog-34.wav\n",
      "cow-76.wav\n",
      "sheep-54.wav\n",
      "sheep-96.wav\n",
      "cow-34.wav\n",
      "monkey-86.wav\n",
      "monkey-70.wav\n",
      "cat-51.wav\n",
      "cat-9.wav\n",
      "sheep-48.wav\n",
      "donkey-81.wav\n",
      "sheep-65.wav\n",
      "sheep-64.wav\n",
      "monkey-23.wav\n",
      "cow-30.wav\n",
      "cow-2.wav\n",
      "cat-72.wav\n",
      "cat-28.wav\n",
      "cow-94.wav\n",
      "sheep-84.wav\n",
      "monkey-33.wav\n",
      "dog-88.wav\n",
      "cat-66.wav\n",
      "cat-12.wav\n",
      "monkey-46.wav\n",
      "sheep-43.wav\n",
      "cow-17.wav\n",
      "monkey-89.wav\n",
      "donkey-10.wav\n",
      "monkey-98.wav\n",
      "cat-43.wav\n",
      "dog-6.wav\n",
      "donkey-6.wav\n",
      "cat-89.wav\n",
      "cat-14.wav\n",
      "dog-24.wav\n",
      "donkey-80.wav\n",
      "cow-95.wav\n",
      "cow-54.wav\n",
      "donkey-85.wav\n",
      "cow-63.wav\n",
      "cow-68.wav\n",
      "donkey-48.wav\n",
      "dog-80.wav\n",
      "monkey-62.wav\n",
      "dog-95.wav\n",
      "monkey-69.wav\n",
      "sheep-72.wav\n",
      "monkey-16.wav\n",
      "cat-90.wav\n",
      "cow-38.wav\n",
      "dog-85.wav\n",
      "cow-21.wav\n",
      "dog-28.wav\n",
      "monkey-2.wav\n",
      "dog-58.wav\n",
      "monkey-64.wav\n",
      "donkey-25.wav\n",
      "cow-13.wav\n",
      "cow-59.wav\n",
      "cow-43.wav\n",
      "cow-50.wav\n",
      "monkey-36.wav\n",
      "sheep-28.wav\n",
      "donkey-98.wav\n",
      "monkey-1.wav\n",
      "monkey-19.wav\n",
      "dog-42.wav\n",
      "donkey-44.wav\n",
      "cow-62.wav\n",
      "cat-41.wav\n",
      "cat-25.wav\n",
      "donkey-45.wav\n"
     ]
    }
   ],
   "source": [
    "#Split the dataset\n",
    "import librosa\n",
    "import numpy as np\n",
    "x_train,x_test,y_trai,y_tes=load_data(test_size=0.33)\n",
    "print(np.shape(x_train),np.shape(x_test), np.shape(y_trai),np.shape(y_tes))\n",
    "y_test_map = np.array(y_tes).T\n",
    "y_test = y_test_map[0]\n",
    "test_filename = y_test_map[1]\n",
    "y_train_map = np.array(y_trai).T\n",
    "y_train = y_train_map[0]\n",
    "train_filename = y_train_map[1]\n",
    "print(np.shape(y_train),np.shape(y_test))\n",
    "print(*test_filename,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gsjYHMvxPF3W",
    "outputId": "d8bfc882-1953-4506-a940-6031c8eaa5e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted: 53\n"
     ]
    }
   ],
   "source": [
    "#Get the shape of the training and testing datasets\n",
    "#print((x_train, x_test))\n",
    "#Get the number of features extracted\n",
    "print(f'Features extracted: {x_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "71es215WPF5W"
   },
   "outputs": [],
   "source": [
    "# Initialize the Multi Layer Perceptron Classifier\n",
    "model=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7n88os-SPF7Q",
    "outputId": "822f50c9-2b19-4b46-dd5c-ce0f45fc5b54"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,),\n",
       "              learning_rate=&#x27;adaptive&#x27;, max_iter=500)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,),\n",
       "              learning_rate=&#x27;adaptive&#x27;, max_iter=500)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,),\n",
       "              learning_rate='adaptive', max_iter=500)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "1-DEkCoLPF9O"
   },
   "outputs": [],
   "source": [
    "#SAVING THE MODEL\n",
    "import pickle\n",
    "# Save the Modle to file in the current working directory\n",
    "#For any new testing data other than the data in dataset\n",
    "\n",
    "Pkl_Filename = \"model.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yHP0ONniPGCH",
    "outputId": "a4329de9-1a3a-454e-9add-87e4e5f6c874"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,),\n",
       "              learning_rate=&#x27;adaptive&#x27;, max_iter=500)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,),\n",
       "              learning_rate=&#x27;adaptive&#x27;, max_iter=500)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,),\n",
       "              learning_rate='adaptive', max_iter=500)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Model back from file\n",
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    animal_voice_classification = pickle.load(file)\n",
    "\n",
    "animal_voice_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EjOWuHgjPGEg",
    "outputId": "83d535ab-2375-4b78-8560-8890a4391e29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.74523682e+02,  5.27017136e+01, -1.07387856e+02, ...,\n",
       "         7.24989057e-01,  6.08520269e-01,  0.00000000e+00],\n",
       "       [-9.50216064e+01,  1.14300842e+02, -1.55699127e+02, ...,\n",
       "         5.44866741e-01,  4.86567050e-01,  0.00000000e+00],\n",
       "       [-1.34150192e+02,  1.09540070e+02,  2.73901176e+01, ...,\n",
       "         6.30894959e-01,  6.87996984e-01,  0.00000000e+00],\n",
       "       ...,\n",
       "       [-2.95451752e+02,  1.51455185e+02, -4.77531166e+01, ...,\n",
       "         1.50782406e-01,  4.12064731e-01,  0.00000000e+00],\n",
       "       [-1.84664459e+02,  1.63794693e+02, -8.35135956e+01, ...,\n",
       "         4.42214340e-01,  4.57932085e-01,  0.00000000e+00],\n",
       "       [-1.82501755e+02,  1.07746399e+02, -7.91311417e+01, ...,\n",
       "         7.68193781e-01,  8.11506808e-01,  0.00000000e+00]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "msyGgBgaPGIi",
    "outputId": "c6e7c741-e58e-4cb4-fc70-cdb4de7b3ded"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sheep', 'Cow', 'Cat', 'Cow', 'Sheep', 'Cat', 'Monkey', 'Cat',\n",
       "       'Dog', 'Monkey', 'Dog', 'Monkey', 'Cow', 'Donkey', 'Sheep',\n",
       "       'Donkey', 'Dog', 'Cat', 'Sheep', 'Cow', 'Cat', 'Monkey', 'Sheep',\n",
       "       'Cat', 'Dog', 'Monkey', 'Sheep', 'Cat', 'Sheep', 'Sheep', 'Cow',\n",
       "       'Cat', 'Cat', 'Dog', 'Dog', 'Dog', 'Dog', 'Donkey', 'Cat', 'Cow',\n",
       "       'Monkey', 'Sheep', 'Cat', 'Cow', 'Monkey', 'Cow', 'Donkey', 'Cat',\n",
       "       'Donkey', 'Donkey', 'Cat', 'Cat', 'Donkey', 'Cow', 'Sheep',\n",
       "       'Sheep', 'Sheep', 'Dog', 'Sheep', 'Sheep', 'Dog', 'Dog', 'Cat',\n",
       "       'Cat', 'Cat', 'Monkey', 'Cat', 'Sheep', 'Cat', 'Cow', 'Donkey',\n",
       "       'Dog', 'Dog', 'Donkey', 'Donkey', 'Donkey', 'Cow', 'Dog', 'Cow',\n",
       "       'Sheep', 'Sheep', 'Cow', 'Monkey', 'Monkey', 'Cat', 'Cat', 'Sheep',\n",
       "       'Donkey', 'Sheep', 'Sheep', 'Monkey', 'Cow', 'Cow', 'Cat', 'Cat',\n",
       "       'Cow', 'Sheep', 'Monkey', 'Dog', 'Dog', 'Cat', 'Monkey', 'Sheep',\n",
       "       'Cow', 'Monkey', 'Donkey', 'Monkey', 'Cat', 'Dog', 'Donkey', 'Cat',\n",
       "       'Cat', 'Dog', 'Donkey', 'Cow', 'Cow', 'Donkey', 'Cow', 'Cow',\n",
       "       'Donkey', 'Dog', 'Monkey', 'Dog', 'Monkey', 'Sheep', 'Monkey',\n",
       "       'Cat', 'Cow', 'Dog', 'Cow', 'Dog', 'Monkey', 'Dog', 'Monkey',\n",
       "       'Donkey', 'Cow', 'Cow', 'Cow', 'Cow', 'Monkey', 'Sheep', 'Donkey',\n",
       "       'Monkey', 'Monkey', 'Dog', 'Donkey', 'Cow', 'Cat', 'Cat', 'Donkey',\n",
       "       'Monkey', 'Monkey', 'Sheep', 'Cat', 'Monkey', 'Cow', 'Cat',\n",
       "       'Monkey', 'Donkey', 'Cow', 'Dog', 'Donkey', 'Sheep', 'Donkey',\n",
       "       'Donkey', 'Monkey', 'Sheep', 'Cat', 'Monkey', 'Cat', 'Cow', 'Dog',\n",
       "       'Cat', 'Monkey', 'Sheep', 'Donkey', 'Monkey', 'Cat', 'Cow',\n",
       "       'Monkey', 'Cat', 'Donkey', 'Donkey', 'Dog', 'Dog', 'Dog', 'Dog',\n",
       "       'Dog', 'Sheep', 'Sheep', 'Sheep', 'Dog', 'Cat', 'Dog', 'Cow',\n",
       "       'Dog', 'Cow', 'Sheep'], dtype='<U6')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicting :\n",
    "y_pred=animal_voice_classification.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "evdwLUwX83qr",
    "outputId": "da8b472b-712b-40e2-c811-43de8f127d58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    predictions    file_names\n",
      "0         Sheep  sheep-38.wav\n",
      "1           Cow    cow-86.wav\n",
      "2           Cat    cat-63.wav\n",
      "3           Cow    cow-46.wav\n",
      "4         Sheep  sheep-55.wav\n",
      "..          ...           ...\n",
      "193         Dog    dog-70.wav\n",
      "194         Cow    cow-29.wav\n",
      "195         Dog    dog-62.wav\n",
      "196         Cow    cow-93.wav\n",
      "197       Sheep  sheep-80.wav\n",
      "\n",
      "[198 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Store the Prediction probabilities into CSV file \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "y_pred1 = pd.DataFrame(y_pred, columns=['predictions'])\n",
    "y_pred1['file_names'] = test_filename\n",
    "print(y_pred1)\n",
    "y_pred1.to_csv('predictionfinal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "Nl37HJsDE7J8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9696969696969697"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
